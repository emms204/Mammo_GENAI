{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c645797-3a67-488d-bfc1-28f66cc4e884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0f0563-4a63-40df-93b7-75f0127b8c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emory = pd.read_csv('emory_final.csv')\n",
    "vini = pd.read_csv('vinidir_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6437543-2072-431f-a19d-a43ff8b87568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emory = emory[~emory.category.isnull()].reset_index(drop=True)\n",
    "emory = emory.rename(columns={'Image_Paths':'image_paths'})\n",
    "emory['image_paths'] = emory['image_paths'].str.replace(r'^../', '', regex=True)\n",
    "emory = emory.rename(columns={'asses': 'breast_birads', 'ViewPosition':'view','ImageLateralityFinal':'laterality'})\n",
    "find_emory = emory[emory['num_roi']!=0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3464383b-14e7-414d-b542-7691f6b28c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vini = vini[vini.category!='No Finding'].reset_index(drop=True)\n",
    "vini['image_paths'] = vini['image_paths'].str.replace(r'^../', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee2f418-2c13-425a-a2af-fbeebe05faad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def torch_CountUpContinuingOnes(b_arr):\n",
    "    left = torch.arange(len(b_arr))\n",
    "    left[b_arr > 0] = 0\n",
    "    left = torch.cummax(left, dim=-1)[0]\n",
    "\n",
    "    rev_arr = torch.flip(b_arr, [-1])\n",
    "    right = torch.arange(len(rev_arr))\n",
    "    right[rev_arr > 0] = 0\n",
    "    right = torch.cummax(right, dim=-1)[0]\n",
    "    right = len(rev_arr) - 1 - torch.flip(right, [-1])\n",
    "\n",
    "    return right - left - 1\n",
    "\n",
    "def torch_ExtractBreast_with_padding_single_side(img_ori, target_size=(512, 512), padding=1):\n",
    "    # Detect background and set to zero\n",
    "    img = torch.where(img_ori <= 20, torch.zeros_like(img_ori), img_ori)\n",
    "    height, _ = img.shape\n",
    "\n",
    "    # Extract the main breast region (same as before)\n",
    "    y_a = height // 2 + int(height * 0.4)\n",
    "    y_b = height // 2 - int(height * 0.4)\n",
    "    b_arr = img[y_b:y_a].to(torch.float32).std(dim=0) != 0\n",
    "    continuing_ones = torch_CountUpContinuingOnes(b_arr)\n",
    "    col_ind = torch.where(continuing_ones == continuing_ones.max())[0]\n",
    "    img = img[:, col_ind]\n",
    "\n",
    "    _, width = img.shape\n",
    "    x_a = width // 2 + int(width * 0.4)\n",
    "    x_b = width // 2 - int(width * 0.4)\n",
    "    b_arr = img[:, x_b:x_a].to(torch.float32).std(dim=1) != 0\n",
    "    continuing_ones = torch_CountUpContinuingOnes(b_arr)\n",
    "    row_ind = torch.where(continuing_ones == continuing_ones.max())[0]\n",
    "    breast_region = img_ori[row_ind][:, col_ind]\n",
    "\n",
    "    # Resize the extracted breast region while maintaining the aspect ratio\n",
    "    breast_height, breast_width = breast_region.shape\n",
    "    aspect_ratio = breast_width / breast_height\n",
    "\n",
    "    # Define target dimensions based on aspect ratio\n",
    "    if aspect_ratio > 1:\n",
    "        # Wider than tall\n",
    "        new_width = target_size[1] - padding\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        # Taller than wide\n",
    "        new_height = target_size[0] - padding\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    resized_breast = cv2.resize(breast_region.cpu().numpy(), (new_width, new_height))\n",
    "    resized_breast = torch.from_numpy(resized_breast)\n",
    "\n",
    "    # Determine which side has lower intensity\n",
    "    pad_x = target_size[1] - new_width\n",
    "    pad_y = target_size[0] - new_height\n",
    "\n",
    "    # Initialize offsets\n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "    left_intensity = 0\n",
    "    right_intensity = 0\n",
    "    top_intensity = 0\n",
    "    bottom_intensity = 0\n",
    "\n",
    "    # Decide padding side for x-axis\n",
    "    if pad_x > 0:\n",
    "        left_intensity = resized_breast[:, 0].mean()\n",
    "        right_intensity = resized_breast[:, -1].mean()\n",
    "        if left_intensity < right_intensity:\n",
    "            # Pad on the left side\n",
    "            x_offset = pad_x\n",
    "        else:\n",
    "            # Pad on the right side\n",
    "            x_offset = 0\n",
    "\n",
    "    # Decide padding side for y-axis\n",
    "    if pad_y > 0:\n",
    "        top_intensity = resized_breast[0, :].mean()\n",
    "        bottom_intensity = resized_breast[-1, :].mean()\n",
    "        if top_intensity < bottom_intensity:\n",
    "            # Pad on the top side\n",
    "            y_offset = pad_y\n",
    "        else:\n",
    "            # Pad on the bottom side\n",
    "            y_offset = 0\n",
    "\n",
    "    # Create a padded image with the target size and place the resized breast region\n",
    "    padded_img = torch.zeros(target_size, dtype=resized_breast.dtype)\n",
    "    padded_img[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_breast\n",
    "    \n",
    "    data = {'padded_img':padded_img,\n",
    "            'breast_region': breast_region,\n",
    "            'left_intensity': left_intensity,\n",
    "            'right_intensity': right_intensity,\n",
    "            'top_intensity': top_intensity,\n",
    "            'bottom_intensity': bottom_intensity,\n",
    "            'target_size': target_size,\n",
    "            'padding': padding\n",
    "           }\n",
    "\n",
    "    return data\n",
    "\n",
    "# Apply the function in display_grid_transform for Emory dataset\n",
    "def resize_with_padding_vini(img_np, target_size=(512, 512), padding=1):\n",
    "    # Convert image to a torch tensor if not already\n",
    "    img_torch = torch.from_numpy(img_np).to(torch.float32)\n",
    "\n",
    "    # Ensure the breast region fits within a consistent scale\n",
    "    breast_height, breast_width = img_torch.shape\n",
    "    aspect_ratio = breast_width / breast_height\n",
    "\n",
    "    # Define target dimensions based on aspect ratio\n",
    "    if aspect_ratio > 1:\n",
    "        # Wider than tall\n",
    "        new_width = target_size[1] - padding\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        # Taller than wide\n",
    "        new_height = target_size[0] - padding\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "    \n",
    "    # Resize while maintaining the aspect ratio\n",
    "    resized_breast = cv2.resize(img_np, (new_width, new_height))\n",
    "    resized_breast = torch.from_numpy(resized_breast)\n",
    "\n",
    "     # Determine which side has lower intensity\n",
    "    pad_x = target_size[1] - new_width\n",
    "    pad_y = target_size[0] - new_height\n",
    "\n",
    "    # Initialize offsets\n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "    left_intensity = 0\n",
    "    right_intensity = 0\n",
    "    top_intensity = 0\n",
    "    bottom_intensity = 0\n",
    "\n",
    "    # Decide padding side for x-axis\n",
    "    if pad_x > 0:\n",
    "        left_intensity = resized_breast[:, 0].mean()\n",
    "        right_intensity = resized_breast[:, -1].mean()\n",
    "        if left_intensity < right_intensity:\n",
    "            # Pad on the left side\n",
    "            x_offset = pad_x\n",
    "        else:\n",
    "            # Pad on the right side\n",
    "            x_offset = 0\n",
    "\n",
    "    # Decide padding side for y-axis\n",
    "    if pad_y > 0:\n",
    "        top_intensity = resized_breast[0, :].mean()\n",
    "        bottom_intensity = resized_breast[-1, :].mean()\n",
    "        if top_intensity < bottom_intensity:\n",
    "            # Pad on the top side\n",
    "            y_offset = pad_y\n",
    "        else:\n",
    "            # Pad on the bottom side\n",
    "            y_offset = 0\n",
    "\n",
    "    # Create a padded image with the target size and place the resized breast region\n",
    "    padded_img = torch.zeros(target_size, dtype=resized_breast.dtype)\n",
    "    padded_img[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_breast\n",
    "\n",
    "    data = {'padded_img':padded_img,\n",
    "        'breast_region': img_torch,\n",
    "        'left_intensity': left_intensity,\n",
    "        'right_intensity': right_intensity,\n",
    "        'top_intensity': top_intensity,\n",
    "        'bottom_intensity': bottom_intensity,\n",
    "        'target_size': target_size,\n",
    "        'padding': padding\n",
    "       }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0288b190-d3eb-4570-b0b3-940ca4a52f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_clahe(image):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    equalized_img = clahe.apply(image)\n",
    "    return equalized_img\n",
    "\n",
    "def mean_variance_normalization(img_torch):\n",
    "    img_min, img_max = img_torch.min(), img_torch.max()\n",
    "    img_torch = (img_torch - img_min) / (img_max - img_min) * 255\n",
    "    return img_torch\n",
    "\n",
    "def transform_bbox(original_bbox, data):\n",
    "    \"\"\"\n",
    "    Transform bounding box coordinates from original image space to resized and padded space.\n",
    "    \n",
    "    Args:\n",
    "        original_bbox (tuple): (x1, y1, x2, y2) in original image space (3500x2680)\n",
    "    Returns:\n",
    "        tuple: Transformed (x1, y1, x2, y2) coordinates in the new image space\n",
    "    \"\"\"\n",
    "    orig_x1, orig_y1, orig_x2, orig_y2 = original_bbox\n",
    "    \n",
    "    breast_region_shape = data['breast_region'].shape\n",
    "    breast_height, breast_width = breast_region_shape\n",
    "    target_size = data['target_size']\n",
    "    padding = data['padding']\n",
    "    left_intensity = data['left_intensity']\n",
    "    right_intensity = data['right_intensity']\n",
    "    top_intensity = data['top_intensity']\n",
    "    bottom_intensity = data['bottom_intensity']\n",
    "    \n",
    "    # Calculate aspect ratio of the breast region\n",
    "    aspect_ratio = breast_width / breast_height\n",
    "    \n",
    "    # Determine the dimensions after resize (before padding)\n",
    "    if aspect_ratio > 1:\n",
    "        # Wider than tall\n",
    "        new_width = target_size[1] - padding\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        # Taller than wide\n",
    "        new_height = target_size[0] - padding\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "    \n",
    "    # Calculate scaling factors\n",
    "    scale_x = new_width / breast_width\n",
    "    scale_y = new_height / breast_height\n",
    "    \n",
    "    # Scale the bbox coordinates\n",
    "    scaled_x1 = int(orig_x1 * scale_x)\n",
    "    scaled_y1 = int(orig_y1 * scale_y)\n",
    "    scaled_x2 = int(orig_x2 * scale_x)\n",
    "    scaled_y2 = int(orig_y2 * scale_y)\n",
    "    \n",
    "    # Calculate padding offsets based on image dimensions\n",
    "    pad_x = target_size[1] - new_width\n",
    "    pad_y = target_size[0] - new_height\n",
    "    \n",
    "    # Get the intensity values from the edges of the breast region\n",
    "    # You'll need to pass these values or compute them here\n",
    "    # For now, we'll assume they're computed the same way as in your original function\n",
    "    x_offset = pad_x if left_intensity < right_intensity else 0\n",
    "    y_offset = pad_y if top_intensity < bottom_intensity else 0\n",
    "    \n",
    "    # Apply offsets to the scaled coordinates\n",
    "    final_x1 = scaled_x1 + x_offset\n",
    "    final_y1 = scaled_y1 + y_offset\n",
    "    final_x2 = scaled_x2 + x_offset\n",
    "    final_y2 = scaled_y2 + y_offset\n",
    "    \n",
    "    # Ensure coordinates are within bounds\n",
    "    final_x1 = max(0, min(final_x1, target_size[1]-1))\n",
    "    final_y1 = max(0, min(final_y1, target_size[0]-1))\n",
    "    final_x2 = max(0, min(final_x2, target_size[1]-1))\n",
    "    final_y2 = max(0, min(final_y2, target_size[0]-1))\n",
    "    \n",
    "    return (final_x1, final_y1, final_x2, final_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6faf032f-fc24-4cfa-9acd-60c2f49d238a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(category\n",
       " Asymmetry                   139\n",
       " Suspicious Calcification     80\n",
       " Mass                         31\n",
       " Architectural distortion     15\n",
       " Name: count, dtype: int64,\n",
       " category\n",
       " Mass                        1226\n",
       " Suspicious Calcification     453\n",
       " Focal Asymmetry              236\n",
       " Architectural Distortion      96\n",
       " Asymmetry                     92\n",
       " Suspicious Lymph Node         57\n",
       " Skin Thickening               49\n",
       " Global Asymmetry              24\n",
       " Nipple Retraction             14\n",
       " Skin Retraction                7\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_emory['category'].value_counts(), vini['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c611c2-cb92-4150-881f-756d1e89882b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db55a8b1-8774-4a2c-8e7e-1c30502289b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_emory = find_emory.drop([12], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8137437d-c98b-4895-9268-a1cd3bd7ae74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_emory = train_emory[train_emory['category']!='Architectural distortion'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e62c73-c071-4e8b-9249-66bd72d77340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_map = {'Asymmetry': 0, 'Suspicious Calcification': 1, 'Mass': 2}\n",
    "train_emory['category'] = train_emory['category'].map(cat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370bcf2a-7f59-4bfd-a026-4dd842ef420b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "category_mapping = {\n",
    "    'asymmetry': 0,\n",
    "    'focal asymmetry':0,\n",
    "    'global asymmetry':0,\n",
    "    'architectural distortion':0,\n",
    "    'nipple retraction':0,\n",
    "    'skin retraction':0,\n",
    "    'suspicious calcification':1,\n",
    "    'mass':2,\n",
    "    'suspicious lymph node':2,\n",
    "    'skin thickening':2,\n",
    "}\n",
    "\n",
    "train_vini = vini.copy()\n",
    "train_vini['category'] = train_vini['category'].str.lower()\n",
    "train_vini['category'] = train_vini['category'].map(category_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70ba3604-6d9a-4ad7-8195-f73acc6e6ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "traine, vale = train_test_split(train_emory, test_size=0.1, stratify=train_emory['category'])\n",
    "traine, vale = traine.reset_index(drop=True), vale.reset_index(drop=True)\n",
    "\n",
    "trainv, valv = train_test_split(train_vini, test_size=0.1, stratify=train_vini['category'])\n",
    "trainv, valv = trainv.reset_index(drop=True), valv.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05b87fcd-cca8-412d-82d5-4fbcd2ef44ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('comb_yolo1/train/images', exist_ok=True)\n",
    "os.makedirs('comb_yolo1/train/labels', exist_ok=True)\n",
    "\n",
    "os.makedirs('comb_yolo1/val/images', exist_ok=True)\n",
    "os.makedirs('comb_yolo1/val/labels', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c646f78f-0256-437d-aa9e-74210b1559ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bboxes_yolo(data, dst_path, label_path, emory=True):\n",
    "    for i, row in tqdm(enumerate(data.itertuples()), desc=\"Getting BBoxes..\"):\n",
    "        \n",
    "        img_file_path = row.image_paths\n",
    "        if os.path.exists(img_file_path):\n",
    "            \n",
    "            try:\n",
    "                img_np = cv2.imread(img_file_path, cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                if emory:\n",
    "                    img_torch = torch.from_numpy(img_np).to(torch.float32)\n",
    "                    img_torch = mean_variance_normalization(img_torch)\n",
    "                    image_data = torch_ExtractBreast_with_padding_single_side(img_torch, padding=0) \n",
    "                \n",
    "                else:\n",
    "                    img_np = mean_variance_normalization(img_np)\n",
    "                    image_data = resize_with_padding_vini(img_np, padding=0)\n",
    "                # Convert the result back to a NumPy array for visualization or further processing\n",
    "                padded_img_np = image_data['padded_img'].cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "                padded_img_rgb = np.repeat(padded_img_np[:, :, np.newaxis], 3, axis=2)\n",
    "                padded_image = Image.fromarray(padded_img_rgb)\n",
    "                filename = f\"emory_{row.image_paths.split('/')[-1]}\" if emory else f\"vini_{row.image_paths.split('/')[-1]}\"\n",
    "                dst_img_path = os.path.join(dst_path, filename)\n",
    "                padded_image.save(dst_img_path)\n",
    "                \n",
    "                labelname = f\"emory_{row.image_paths.split('/')[-1].replace('.png','.txt')}\" if emory else f\"vini_{row.image_paths.split('/')[-1].replace('.png','.txt')}\"\n",
    "                label_file_path = os.path.join(label_path, labelname)\n",
    "                width, height = padded_image.size\n",
    "                \n",
    "                if emory:\n",
    "                    for roi in ast.literal_eval(row.ROI_coords):\n",
    "                        y1, x1, y2, x2 = roi\n",
    "                        # print(f\"OLD ROI: x1:{x1}, y1:{y1}, x2:{x2}, y2:{y2}\")\n",
    "                        adj_roi=transform_bbox((x1, y1, x2, y2), image_data)\n",
    "                        x1, y1, x2, y2 = adj_roi\n",
    "                        # print(f\"ADJ ROI: x1:{x1}, y1:{y1}, x2:{x2}, y2:{y2}\")\n",
    "\n",
    "                        x_center = ((x1 + x2) / 2) / width\n",
    "                        y_center = ((y1 + y2) / 2) / height\n",
    "                        wid = (x2 - x1) / width\n",
    "                        hei = (y2 - y1) / height\n",
    "\n",
    "                        text = [row.category, x_center, y_center, wid, hei]\n",
    "                        with open(label_file_path, 'w') as f:\n",
    "                            text = [str(i) for i in text]\n",
    "                            f.write(' '.join(text))\n",
    "                    \n",
    "                else:\n",
    "                    x1, y1, x2, y2 = row.resized_xmin, row.resized_ymin, row.resized_xmax, row.resized_ymax\n",
    "                    adj_roi=transform_bbox((x1, y1, x2, y2), image_data)\n",
    "                    x1, y1, x2, y2 = adj_roi\n",
    "\n",
    "                    x_center = ((x1 + x2) / 2) / width\n",
    "                    y_center = ((y1 + y2) / 2) / height\n",
    "                    wid = (x2 - x1) / width\n",
    "                    hei = (y2 - y1) / height\n",
    "\n",
    "                    text = [row.category, x_center, y_center, wid, hei]\n",
    "                    with open(label_file_path, 'w') as f:\n",
    "                        text = [str(i) for i in text]\n",
    "                        f.write(' '.join(text))\n",
    "            except Exception as e:\n",
    "                print(f\"Cannot open File {img_file_path} because of exception{e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"FILE NOT FOUND {img_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ecd0651-65f6-4518-bfb6-1183a5b9a71e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bboxes_synth_yolo(class_data_dir, dst_path, label_path, ratio=1.0):\n",
    "    \"\"\"\n",
    "    Processes synthetic images and saves a specified ratio of them along with their YOLO format labels.\n",
    "\n",
    "    Args:\n",
    "        class_data_dir (str): Path to the directory containing synthetic lesion classes.\n",
    "        dst_path (str): Path to save processed images.\n",
    "        label_path (str): Path to save YOLO labels.\n",
    "        ratio (float): Ratio of images to process (0.0 to 1.0).\n",
    "    \"\"\"\n",
    "    # Validate ratio\n",
    "    if not (0.0 < ratio <= 1.0):\n",
    "        raise ValueError(\"Ratio must be between 0.0 and 1.0\")\n",
    "\n",
    "    # Retrieve all synthetic images and corresponding ROIs across the 3 lesions\n",
    "    lesion_dirs = {\n",
    "        \"Asymmetry\": os.path.join(class_data_dir, \"asymmetry\"),\n",
    "        \"Suspicious Calcification\": os.path.join(class_data_dir, \"suspicious calcification\"),\n",
    "        \"Mass\": os.path.join(class_data_dir, \"mass\")\n",
    "    }\n",
    "    \n",
    "    # Load images and ROIs\n",
    "    image_roi_pairs = []\n",
    "    for lesion_name, lesion_dir in lesion_dirs.items():\n",
    "        roi_path = os.path.join(lesion_dir, \"generated_rois.json\")\n",
    "        with open(roi_path, \"r\") as f:\n",
    "            rois = json.load(f)\n",
    "\n",
    "        lesion_images = sorted([os.path.join(lesion_dir, img) for img in os.listdir(lesion_dir) if img.endswith(\".png\")])\n",
    "        if len(lesion_images) != len(rois):\n",
    "            print(f\"Warning: Number of images and ROIs do not match in {lesion_name}.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the number of images to process based on the ratio\n",
    "        num_images_to_process = int(len(lesion_images) * ratio)\n",
    "        sampled_pairs = random.sample(list(zip(lesion_images, rois)), num_images_to_process)\n",
    "        image_roi_pairs.extend([(img, roi, lesion_name) for img, roi in sampled_pairs])\n",
    "\n",
    "    # Shuffle the images and corresponding ROIs\n",
    "    random.shuffle(image_roi_pairs)\n",
    "\n",
    "    # Process and copy images to the destination path\n",
    "    for i, (img_path, roi, lesion_name) in tqdm(enumerate(image_roi_pairs), desc=\"Processing Images\", unit=\"image\"):\n",
    "        try:\n",
    "            # Build destination file paths\n",
    "            image_id = f\"{class_data_dir.split('_')[-1]}_{lesion_name.lower()}_{i:06d}\"\n",
    "            dst_img_path = os.path.join(dst_path, f\"{image_id}.png\")\n",
    "            label_file_path = os.path.join(label_path, f\"{image_id}.txt\")\n",
    "\n",
    "            # Open and process the image\n",
    "            if os.path.exists(img_path):\n",
    "                img = Image.open(img_path)\n",
    "                shutil.copy(img_path, dst_img_path)\n",
    "                width, height = img.size\n",
    "                x1, y1, x2, y2 = roi\n",
    "\n",
    "                # Convert bounding box to YOLO format\n",
    "                x_center = ((x1 + x2) / 2) / width\n",
    "                y_center = ((y1 + y2) / 2) / height\n",
    "                bbox_width = (x2 - x1) / width\n",
    "                bbox_height = (y2 - y1) / height\n",
    "\n",
    "                # Write label file in YOLO format\n",
    "                category = list(cat_map.keys()).index(lesion_name) # Get category index\n",
    "                label_text = f\"{category} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\"\n",
    "                with open(label_file_path, 'w') as f:\n",
    "                    f.write(label_text)\n",
    "            else:\n",
    "                print(f\"Image file not found: {img_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    print(f\"Finished processing {len(image_roi_pairs)} images. Images and labels saved to {dst_path} and {label_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b53afe62-8fef-4319-8ed8-ab25990b6df2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543a1b057fe441e0af39b291cdc7d27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting BBoxes..: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot open File emory/images_png/16168291/1.2.845.113682.2750824972.1550469931.4661.2139.1/38180941880001850727199742295180123832.png because of exceptionexpected np.ndarray (got NoneType)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9467dc49fdfb479b92d405abc20835c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting BBoxes..: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518791b141f94ae4b6888d67c8eb76ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting BBoxes..: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3f7c3acd434bafbcfe91a70622fafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting BBoxes..: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_bboxes_yolo(traine, dst_path=\"comb_yolo1/train/images\", label_path=\"comb_yolo1/train/labels\")\n",
    "get_bboxes_yolo(vale,  dst_path=\"comb_yolo1/val/images\", label_path=\"comb_yolo1/val/labels\")\n",
    "\n",
    "get_bboxes_yolo(trainv, dst_path=\"comb_yolo1/train/images\", label_path=\"comb_yolo1/train/labels\", emory=False)\n",
    "get_bboxes_yolo(valv,  dst_path=\"comb_yolo1/val/images\", label_path=\"comb_yolo1/val/labels\", emory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "045d0224-1e13-42ae-a26b-35e7d419b5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788 1788\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(\"comb_yolo1/train/images\")), len(os.listdir(\"comb_yolo1/train/labels\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cdcddac-d9d7-4fe8-be5d-85c6ffcdda25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f7206c7d6b4ae4a15e56bba4fc5f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Images: 0image [00:00, ?image/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 2250 images. Images and labels saved to comb_yolo1/train/images and comb_yolo1/train/labels.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a0ee30cf7a41c28b5a9161c59a3b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Images: 0image [00:00, ?image/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 2250 images. Images and labels saved to comb_yolo1/train/images and comb_yolo1/train/labels.\n"
     ]
    }
   ],
   "source": [
    "get_bboxes_synth_yolo(class_data_dir=\"synthetic_lesions\",dst_path=\"comb_yolo1/train/images\",label_path=\"comb_yolo1/train/labels\", ratio=0.375)\n",
    "get_bboxes_synth_yolo(class_data_dir=\"synthetic_vlesions\",dst_path=\"comb_yolo1/train/images\",label_path=\"comb_yolo1/train/labels\", ratio=0.375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86aa8d4d-5cc4-4890-86e6-3530ec90eb7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6288 6288\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(\"comb_yolo1/train/images\")), len(os.listdir(\"comb_yolo1/train/labels\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3870817-b5a9-4bfb-a08a-7710cc730e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dict(\n",
    "    train = \"/notebooks/comb_yolo1/train/images\",\n",
    "    val = \"/notebooks/comb_yolo1/val/images\",\n",
    "\n",
    "    nc = len(cat_map),\n",
    "    names = list(cat_map.keys())\n",
    ")\n",
    "\n",
    "with open('comb_yolo1/data.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b9762ec-af07-41b7-b326-b9ec9f91bb33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5328b6d-5299-476c-a09a-ec6ec1bf5a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yaml_file = 'comb_yolo1/data.yaml'\n",
    "\n",
    "model.train(data=yaml_file,\n",
    "            epochs=50,\n",
    "            patience=20,\n",
    "            batch=32,\n",
    "            optimizer='Adam',\n",
    "            lr0 = 1e-4,\n",
    "            lrf = 1e-3,\n",
    "            weight_decay = 5e-4,\n",
    "            name = f'yolov8s_3classes_comb2_0.5',\n",
    "            save=True,\n",
    "            amp=True,\n",
    "            val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e7d03-0eb0-4a32-acba-caa27f23d1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the directory\n",
    "post_training_files_path = 'runs/detect/yolov8s_3classes_comb2_0.5'\n",
    "\n",
    "# Construct the path to the best model weights file using os.path.join\n",
    "best_model_path = os.path.join(post_training_files_path, 'weights/best.pt')\n",
    "\n",
    "# Load the best model weights into the YOLO model\n",
    "best_model = YOLO(best_model_path)\n",
    "\n",
    "# Validate the best model using the validation set with default parameters\n",
    "metrics = best_model.val(split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b35e1-a193-4ba2-a3c4-ea1c2f2c7e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the dictionary to a pandas DataFrame and use the keys as the index\n",
    "metrics = pd.DataFrame.from_dict(metrics.results_dict, orient='index', columns=['Metric Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e7724-b8f8-4fc1-a129-329fd7fbdb05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the visual appearance of Seaborn plots\n",
    "sns.set(rc={'axes.facecolor': '#9b63b8'}, style='darkgrid')\n",
    "\n",
    "def display_images(post_training_files_path, image_files):\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(post_training_files_path, image_file)\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.figure(figsize=(10, 10), dpi=120)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# List of image files to display\n",
    "image_files = [\n",
    "    'confusion_matrix_normalized.png',\n",
    "    'F1_curve.png',\n",
    "    'P_curve.png',\n",
    "    'R_curve.png',\n",
    "    'PR_curve.png',\n",
    "    'results.png'\n",
    "]\n",
    "\n",
    "# Display the images\n",
    "display_images(post_training_files_path, image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50addd39-b063-4fe0-9eb6-36de982f88c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(mam)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
