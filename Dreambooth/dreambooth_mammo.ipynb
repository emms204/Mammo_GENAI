{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2264fc-eb80-416f-acf6-cb2af4e6c585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effacc91-d022-4727-a89d-68c72d2d4e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# healthy_emory = pd.read_csv('emory_final.csv')\n",
    "healthy_vini = pd.read_csv('vinidir_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f889a65-dd17-4679-bc09-710eb39979db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# healthy_emory = emory[emory.category.isnull()].reset_index(drop=True)\n",
    "# healthy_vini = vini[vini.category=='No Finding'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a55f6-2f22-441c-a8d9-68dc3cf0f2de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# healthy_vini = healthy_vini.rename(columns={'image_paths':'image_path'})\n",
    "# healthy_emory = healthy_emory.rename(columns={'Image_Paths':'image_path'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b326eb-56f0-46b1-bb2f-06339f31ba33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# healthy_emory['image_path'] = healthy_emory['image_path'].str.replace(r'^../', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142f25f-6b01-4e1a-89cb-fd139bf53113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# healthy_emory = healthy_emory[(healthy_emory['asses']!='BIRADS 0') & (healthy_emory['asses']!='BIRADS 6')].reset_index(drop=True)\n",
    "\n",
    "# healthy_emory['asses'] = healthy_emory['asses'].str.replace(' ', '').str.upper()\n",
    "# # vini['breast_birads'] = vini['breast_birads'].str.replace('-', '').str.replace(' ', '').str.upper()\n",
    "\n",
    "# # Rename columns for clarity and merge datasets\n",
    "# healthy_emory = healthy_emory.rename(columns={'asses': 'breast_birads', 'ViewPosition':'view','ImageLateralityFinal':'laterality'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed2750-3019-40e3-85b3-9e7c7aa757ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# healthy_emory.breast_birads.value_counts()\n",
    "healthy_vini.breast_birads.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dcfb29-353c-4ff8-b56c-71c605e02a58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# em1 = emory_f[emory_f['breast_birads']=='BIRADS1'].reset_index(drop=True)\n",
    "# vn1 = healthy_vini[healthy_vini['breast_birads']=='BI-RADS 1'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d7198-1992-436c-af13-943abed79825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # Filter out rows with BIRADS5 in either breast_birads_CC or breast_birads_MLO\n",
    "# # healthy_emory = healthy_emory[~healthy_emory['breast_birads'].isin(['BIRADS5'])]\n",
    "# # Get the count for each level in the combined dataset\n",
    "# level_counts = healthy_emory['breast_birads'].value_counts()\n",
    "\n",
    "# # Determine the minimum count across levels for balanced sampling\n",
    "# min_count = level_counts.min()\n",
    "\n",
    "# # Sample the minimum number of records for each BIRADS level\n",
    "# healthy_emory = healthy_emory.groupby('breast_birads', group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42))\n",
    "\n",
    "# # Verify the balanced distribution\n",
    "# healthy_emory['breast_birads'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224554e-e519-45ea-893f-e1df5da40eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # Filter out rows with BIRADS5 in either breast_birads_CC or breast_birads_MLO\n",
    "# # healthy_emory = healthy_emory[~healthy_emory['breast_birads'].isin(['BIRADS5'])]\n",
    "# # Get the count for each level in the combined dataset\n",
    "# level_counts = healthy_vini['breast_birads'].value_counts()\n",
    "\n",
    "# # Determine the minimum count across levels for balanced sampling\n",
    "# min_count = level_counts.min()\n",
    "\n",
    "# # Sample the minimum number of records for each BIRADS level\n",
    "# healthy_vini = healthy_vini.groupby('breast_birads', group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42))\n",
    "\n",
    "# # Verify the balanced distribution\n",
    "# healthy_vini['breast_birads'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d86c17-5ac5-403d-b711-d8b45322463e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_grid(data):\n",
    "    idx = random.sample(range(data.shape[0]), 4)\n",
    "    \n",
    "    # Set the plot size\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(30, 10))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, row in enumerate(data.iloc[idx].itertuples()):\n",
    "        # Build image path\n",
    "        img_file_path = row.image_paths\n",
    "\n",
    "        # Open and show image\n",
    "        if os.path.exists(img_file_path):\n",
    "            img = Image.open(img_file_path)\n",
    "            try:\n",
    "                axes[i].imshow(img, cmap='gray')\n",
    "            except Exception as e:\n",
    "                print(f\"Cannot Display {img_file_path}, because of error:{e}\")\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, 'Image not found', horizontalalignment='center', verticalalignment='center')\n",
    "        \n",
    "        # Set title with image attributes\n",
    "        title_text = f\"{i} Prompt {row.prompt}\"\n",
    "        axes[i].set_title(title_text, fontsize=8)\n",
    "        # axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1e6b0-a166-45cb-959b-252b99b44f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# em1['breast_density_prompt'] = em1['tissueden'].apply(lambda x: 'very low' if x == 'DENSITY A' else 'low' if x == 'DENSITY B' else 'high' if x == 'DENSITY C' else 'very high')\n",
    "# em1['prompt'] = 'a hologic mammogram in ' + em1['view'] + ' view ' + 'with ' + em1['breast_density_prompt'] + ' density with breast BIRADS 1 Level'\n",
    "\n",
    "# vn1['breast_density_prompt'] = vn1['breast_density'].apply(lambda x: 'very low' if x == 'DENSITY A' else 'low' if x == 'DENSITY B' else 'high' if x == 'DENSITY C' else 'very high')\n",
    "# vn1['prompt'] = 'a siemens mammogram in ' + vn1['view'] + ' view ' + 'with ' + vn1['breast_density_prompt'] + ' density with breast BIRADS 1 Level'\n",
    "\n",
    "# healthy_emory['prompt'] = \"A mammogram\"\n",
    "healthy_vini[\"prompt\"] = \"A mammogram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca6004-fdc3-42eb-b9a2-d95f128180f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display_grid(healthy_emory.iloc[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd594c-ed62-4b2e-ab1f-c252c62b16c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_grid(healthy_vini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d882c89-90f7-4d5e-a14f-d3191cb2b0da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def torch_CountUpContinuingOnes(b_arr):\n",
    "    left = torch.arange(len(b_arr))\n",
    "    left[b_arr > 0] = 0\n",
    "    left = torch.cummax(left, dim=-1)[0]\n",
    "\n",
    "    rev_arr = torch.flip(b_arr, [-1])\n",
    "    right = torch.arange(len(rev_arr))\n",
    "    right[rev_arr > 0] = 0\n",
    "    right = torch.cummax(right, dim=-1)[0]\n",
    "    right = len(rev_arr) - 1 - torch.flip(right, [-1])\n",
    "\n",
    "    return right - left - 1\n",
    "\n",
    "def torch_ExtractBreast_with_padding_single_side(img_ori, target_size=(512, 512), padding=1):\n",
    "    # Detect background and set to zero\n",
    "    img = torch.where(img_ori <= 20, torch.zeros_like(img_ori), img_ori)\n",
    "    height, _ = img.shape\n",
    "\n",
    "    # Extract the main breast region (same as before)\n",
    "    y_a = height // 2 + int(height * 0.4)\n",
    "    y_b = height // 2 - int(height * 0.4)\n",
    "    b_arr = img[y_b:y_a].to(torch.float32).std(dim=0) != 0\n",
    "    continuing_ones = torch_CountUpContinuingOnes(b_arr)\n",
    "    col_ind = torch.where(continuing_ones == continuing_ones.max())[0]\n",
    "    img = img[:, col_ind]\n",
    "\n",
    "    _, width = img.shape\n",
    "    x_a = width // 2 + int(width * 0.4)\n",
    "    x_b = width // 2 - int(width * 0.4)\n",
    "    b_arr = img[:, x_b:x_a].to(torch.float32).std(dim=1) != 0\n",
    "    continuing_ones = torch_CountUpContinuingOnes(b_arr)\n",
    "    row_ind = torch.where(continuing_ones == continuing_ones.max())[0]\n",
    "    breast_region = img_ori[row_ind][:, col_ind]\n",
    "\n",
    "    # Resize the extracted breast region while maintaining the aspect ratio\n",
    "    breast_height, breast_width = breast_region.shape\n",
    "    aspect_ratio = breast_width / breast_height\n",
    "\n",
    "    # Define target dimensions based on aspect ratio\n",
    "    if aspect_ratio > 1:\n",
    "        # Wider than tall\n",
    "        new_width = target_size[1] - padding\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        # Taller than wide\n",
    "        new_height = target_size[0] - padding\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    resized_breast = cv2.resize(breast_region.cpu().numpy(), (new_width, new_height))\n",
    "    resized_breast = torch.from_numpy(resized_breast)\n",
    "\n",
    "    # Determine which side has lower intensity\n",
    "    pad_x = target_size[1] - new_width\n",
    "    pad_y = target_size[0] - new_height\n",
    "\n",
    "    # Initialize offsets\n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "\n",
    "    # Decide padding side for x-axis\n",
    "    if pad_x > 0:\n",
    "        left_intensity = resized_breast[:, 0].mean()\n",
    "        right_intensity = resized_breast[:, -1].mean()\n",
    "        if left_intensity < right_intensity:\n",
    "            # Pad on the left side\n",
    "            x_offset = pad_x\n",
    "        else:\n",
    "            # Pad on the right side\n",
    "            x_offset = 0\n",
    "\n",
    "    # Decide padding side for y-axis\n",
    "    if pad_y > 0:\n",
    "        top_intensity = resized_breast[0, :].mean()\n",
    "        bottom_intensity = resized_breast[-1, :].mean()\n",
    "        if top_intensity < bottom_intensity:\n",
    "            # Pad on the top side\n",
    "            y_offset = pad_y\n",
    "        else:\n",
    "            # Pad on the bottom side\n",
    "            y_offset = 0\n",
    "\n",
    "    # Create a padded image with the target size and place the resized breast region\n",
    "    padded_img = torch.zeros(target_size, dtype=resized_breast.dtype)\n",
    "    padded_img[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_breast\n",
    "\n",
    "    return padded_img\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function in display_grid_transform for Emory dataset\n",
    "\n",
    "def resize_with_padding_vini(img_np, target_size=(512, 512), padding=1):\n",
    "    # Convert image to a torch tensor if not already\n",
    "    img_torch = torch.from_numpy(img_np).to(torch.float32)\n",
    "\n",
    "    # Ensure the breast region fits within a consistent scale\n",
    "    breast_height, breast_width = img_torch.shape\n",
    "    aspect_ratio = breast_width / breast_height\n",
    "\n",
    "    # Define target dimensions based on aspect ratio\n",
    "    if aspect_ratio > 1:\n",
    "        # Wider than tall\n",
    "        new_width = target_size[1] - padding\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        # Taller than wide\n",
    "        new_height = target_size[0] - padding\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "    \n",
    "    # Resize while maintaining the aspect ratio\n",
    "    resized_breast = cv2.resize(img_np, (new_width, new_height))\n",
    "    resized_breast = torch.from_numpy(resized_breast)\n",
    "\n",
    "     # Determine which side has lower intensity\n",
    "    pad_x = target_size[1] - new_width\n",
    "    pad_y = target_size[0] - new_height\n",
    "\n",
    "    # Initialize offsets\n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "\n",
    "    # Decide padding side for x-axis\n",
    "    if pad_x > 0:\n",
    "        left_intensity = resized_breast[:, 0].mean()\n",
    "        right_intensity = resized_breast[:, -1].mean()\n",
    "        if left_intensity < right_intensity:\n",
    "            # Pad on the left side\n",
    "            x_offset = pad_x\n",
    "        else:\n",
    "            # Pad on the right side\n",
    "            x_offset = 0\n",
    "\n",
    "    # Decide padding side for y-axis\n",
    "    if pad_y > 0:\n",
    "        top_intensity = resized_breast[0, :].mean()\n",
    "        bottom_intensity = resized_breast[-1, :].mean()\n",
    "        if top_intensity < bottom_intensity:\n",
    "            # Pad on the top side\n",
    "            y_offset = pad_y\n",
    "        else:\n",
    "            # Pad on the bottom side\n",
    "            y_offset = 0\n",
    "\n",
    "    # Create a padded image with the target size and place the resized breast region\n",
    "    padded_img = torch.zeros(target_size, dtype=resized_breast.dtype)\n",
    "    padded_img[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_breast\n",
    "\n",
    "    return padded_img\n",
    "\n",
    "\n",
    "# Apply in display_grid_transform for the Vini dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ea1df-7385-4730-993a-7d994eb25ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_clahe(image):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    equalized_img = clahe.apply(image)\n",
    "    return equalized_img\n",
    "\n",
    "def mean_variance_normalization(img_torch):\n",
    "    img_min, img_max = img_torch.min(), img_torch.max()\n",
    "    img_torch = (img_torch - img_min) / (img_max - img_min) * 255\n",
    "    return img_torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfabd61-025c-42c8-ab9c-cbfd1348234d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize_image(image_path, size=(512, 512)):\n",
    "    if os.path.exists(image_path):\n",
    "        img_np = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Check if the image was read successfully\n",
    "        if img_np is None:\n",
    "            logging.error(f\"Failed to load image at {image_path}.\")\n",
    "            return None  # You could return a default image or raise an error\n",
    "            \n",
    "        if 'emory' in image_path:\n",
    "            img_torch = torch.from_numpy(img_np).to(torch.float32)\n",
    "            img_torch = mean_variance_normalization(img_torch)\n",
    "\n",
    "            # Extract the breast region\n",
    "            breast_region = torch_ExtractBreast_with_padding_single_side(img_torch)\n",
    "\n",
    "            # Convert the result back to a NumPy array for visualization or further processing\n",
    "            breast_region_np = breast_region.cpu().numpy().astype(np.uint8)\n",
    "            # breast_region_np = apply_clahe(breast_region_np)\n",
    "            breast_region_rgb = np.repeat(breast_region_np[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "            # Convert the NumPy array to a PIL image for compatibility with transforms\n",
    "            breast_region_pil = Image.fromarray(breast_region_rgb)\n",
    "        else:\n",
    "            # img_torch = torch.from_numpy(img_np).to(torch.float32)\n",
    "            # # Normalize the image\n",
    "            img_np = mean_variance_normalization(img_np)\n",
    "            breast_region_np = resize_with_padding_vini(img_np)\n",
    "            # Convert the result back to a NumPy array for visualization or further processing\n",
    "            breast_region_np = breast_region_np.cpu().numpy().astype(np.uint8)\n",
    "            # breast_region_np = apply_clahe(breast_region_np)\n",
    "\n",
    "            breast_region_rgb = np.repeat(breast_region_np[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "            # Convert the NumPy array to a PIL image for compatibility with transforms\n",
    "            breast_region_pil = Image.fromarray(breast_region_rgb)\n",
    "\n",
    "        img = breast_region_pil\n",
    "        return img\n",
    "    else:\n",
    "        logging.error(f\"Image path does not exist: {image_path}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23368765-eef4-4d3b-b3c1-4b3500f78a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "exp_tr = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(512, 512), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5],[0.5])\n",
    "    ]\n",
    ")\n",
    "    \n",
    "def display_grid_transform(data):\n",
    "    idx = random.sample(range(data.shape[0]), 4)\n",
    "    \n",
    "    # Set the plot size\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(30, 10))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, row in enumerate(data.iloc[idx].itertuples()):\n",
    "        # Build image path\n",
    "        img_file_path = row.image_paths\n",
    "\n",
    "        # Open and show image\n",
    "        if os.path.exists(img_file_path):\n",
    "            img = resize_image(img_file_path)\n",
    "            img_t = exp_tr(img)\n",
    "            try:\n",
    "                axes[i].imshow(img_t.numpy().transpose((1,2,0)))\n",
    "            except Exception as e:\n",
    "                print(f\"Cannot Display {img_file_path}, because of error:{e}\")\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, 'Image not found', horizontalalignment='center', verticalalignment='center')\n",
    "        \n",
    "        # Set title with image attributes\n",
    "        title_text = f\"{i} Prompt {row.prompt}\"\n",
    "        axes[i].set_title(title_text, fontsize=8)\n",
    "        # axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10cab9-e8a6-479b-96c5-bbff09c6e52a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display_grid_transform(healthy_emory.iloc[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6abd8cc-d823-44f9-8eeb-02bf14981409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_grid_transform(healthy_vini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7727483-07f3-4815-a790-4d27f24aa091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = pd.concat([\n",
    "#     healthy_emory[['image_path', 'prompt']],\n",
    "#     healthy_vini[['image_path', 'prompt']]\n",
    "# ], axis=0).reset_index(drop=True)\n",
    "data = healthy_vini[['image_paths', 'prompt']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "# Drop rows where 'prompt' is not a string\n",
    "train = train[train['prompt'].apply(lambda x: isinstance(x, str))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe8b1e-1f71-4bef-9b75-c158f322a490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instance_prompt = \"A mammogram\"\n",
    "# validation_prompt = \"A mammogram in CC view with high density\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82577df-e2f3-4e31-b418-cb99c4e7b0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Find all rows where 'prompt' is not a string\n",
    "# non_string_prompts = train[~train['prompt'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# # Display the result\n",
    "# print(non_string_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118b5da-6e95-487e-9d44-166d5b1e5c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class DreamBoothDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset, \n",
    "        tokenizer, \n",
    "        class_data_root=None,\n",
    "        class_prompt=None,\n",
    "        class_num=None,\n",
    "        size=512):\n",
    "        self.dataset = dataset\n",
    "        # self.instance_prompt = instance_prompt\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size=(size, size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.RandomHorizontalFlip(0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if class_data_root is not None: # if there are prior images\n",
    "            self.class_data_root = Path(class_data_root)\n",
    "            self.class_data_root.mkdir(parents=True, exist_ok=True) # create the folder if it doesn't exist\n",
    "            self.class_images_path = [image_path for image_path in self.class_data_root.glob(\"*jpg\")] # get paths of all the class images\n",
    "            if class_num is not None: # class number. This can vary if there are more images in the folder and we only want to use a subset of them\n",
    "                self.num_class_images = min(len(self.class_images_path), class_num)\n",
    "            else:\n",
    "                self.num_class_images = len(self.class_images_path)\n",
    "            self._length = max(self.num_class_images, self.dataset.shape[0]) # length of the dataset will be the max of the number of instance images and the number of class images\n",
    "            self.class_prompt = class_prompt\n",
    "        else:\n",
    "            self.class_data_root = None\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        image_path = self.dataset.image_paths[index]\n",
    "        instance_prompt = self.dataset.prompt[index]\n",
    "\n",
    "        img = resize_image(image_path, (self.size, self.size))\n",
    "\n",
    "        # Retry with a different image or handle the None case\n",
    "        if img is None:\n",
    "            logging.warning(f\"Image loading failed at index {index}. Using a fallback image.\")\n",
    "            image_path = self.dataset.image_path[index-1]\n",
    "            img = resize_image(image_path, (self.size, self.size))\n",
    "            \n",
    "        if img:\n",
    "            img_t = self.transforms(img)\n",
    "\n",
    "        example[\"instance_images\"] = img_t\n",
    "\n",
    "        # Convert tokenizer output to tensor immediately\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            instance_prompt,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        \n",
    "        if self.class_data_root: # if there are class images\n",
    "            class_image = Image.open(self.class_images_path[index % self.num_class_images]) # same idea as above\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.transforms(class_image)\n",
    "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
    "                self.class_prompt,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids\n",
    "            \n",
    "        return example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370b32c-fe76-4355-b5ad-bc5fd5ff94c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The Stable Diffusion checkpoint we'll fine-tune\n",
    "# model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "# model_id = \"results1.5/test\"\n",
    "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91542b-47cb-4f7c-a915-e899b318f661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = DreamBoothDataset(train, tokenizer)\n",
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e004db-4f93-435c-bd71-8c384d70df39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples, with_prior_preservation=False):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "    \n",
    "    if with_prior_preservation:\n",
    "        input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
    "        pixel_values += [example[\"class_images\"] for example in examples]\n",
    "    \n",
    "    # Stack pixel values\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    \n",
    "    # Stack input ids\n",
    "    input_ids = torch.stack(input_ids)  # Changed from torch.cat to torch.stack\n",
    "    \n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"pixel_values\": pixel_values,\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcadd3f-8149-45c0-9348-c676df2ca54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    \"\"\"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\n",
    "    The get item returns example dictionary with the prompt and the index of the example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prompt, num_samples):\n",
    "        \"\"\"init with promt and number of samples\n",
    "\n",
    "        Args:\n",
    "            prompt (str): text prompt\n",
    "            num_samples (int): num of samples\n",
    "        \"\"\"\n",
    "        self.prompt = prompt\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        example[\"prompt\"] = self.prompt\n",
    "        example[\"index\"] = index\n",
    "        return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d383438-f2d3-4cdc-9e42-aa17896db30f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    # revision = 'fp16',\n",
    "    #data\n",
    "    resolution=512, # Reduce this if you want to save some memory\n",
    "    seed=1337,\n",
    "    with_prior_preservation=False,\n",
    "    class_data_dir = None,#\"synthetic_mammograms\",\n",
    "    class_prompt = \"a mammogram\",\n",
    "    num_class_images = 100,\n",
    "    prior_loss_weight = 1.0,\n",
    "    prior_generation_precision = 'fp16',\n",
    "    # revision='fp16',\n",
    "    # instance_prompt=instance_prompt,\n",
    "    \n",
    "    #Training\n",
    "    learning_rate=1e-6,\n",
    "    rank = 4,\n",
    "    train_text_encoder=True,\n",
    "    max_train_steps=1500,\n",
    "    train_batch_size=4,\n",
    "    gradient_accumulation_steps=1, # Increase this if you want to lower memory usage\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,  # Set this to True to lower the memory usage\n",
    "    use_8bit_adam=True,  # Use 8bit optimizer from bitsandbytes\n",
    "    dataloader_num_workers = 8,\n",
    "    lr_scheduler = 'constant',\n",
    "    lr_warmup_steps = 0,\n",
    "    lr_num_cycles = 1, #Number of hard resets of the lr in cosine_with_restarts scheduler\n",
    "    lr_power = 1.0, #Power of the polynomial scheduler\n",
    "    \n",
    "     #memory\n",
    "    enable_xformers_memory_efficient_attention=True,\n",
    "    allow_tf32 = True, # on Ampere GPU only\n",
    "    mixed_precision = 'fp16', # fp16 or bf16\n",
    "    \n",
    "    #Optimizer\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.999,\n",
    "    adam_weight_decay = 1.0e-2,\n",
    "    adam_epsilon = 1.0e-8,\n",
    "    sample_batch_size=4,\n",
    "    output_dir=\"results_vini/test\",  # Where to save the pipeline\n",
    "    \n",
    "    #logging\n",
    "    validation_prompt = \"a mammogram\", # validation prompt for logging\n",
    "    num_validation_images = 4,\n",
    "    validation_steps = 750, # how often to log validation images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933bdd0-ebf5-48c0-99d1-856a5afe8ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "# from transformers import CLIPFeatureExtractor, CLIPTextModel\n",
    "\n",
    "# text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\")\n",
    "# vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n",
    "# unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\n",
    "# # feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721067d-7d2c-4100-8c32-03f1e2c9c23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "import xformers\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    DiffusionPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    UNet2DConditionModel,\n",
    "    DDIMScheduler,\n",
    "    StableDiffusionPipeline\n",
    ")\n",
    "from transformers import AutoTokenizer, PretrainedConfig, CLIPTextModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from packaging import version\n",
    "\n",
    "import wandb\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9c780-d44a-4196-b7e7-6034091d33db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "    )\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "\n",
    "        return CLIPTextModel\n",
    "    elif model_class == \"RobertaSeriesModelWithTransformation\":\n",
    "        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n",
    "\n",
    "        return RobertaSeriesModelWithTransformation\n",
    "    elif model_class == \"T5EncoderModel\":\n",
    "        from transformers import T5EncoderModel\n",
    "\n",
    "        return T5EncoderModel\n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0acc70-8f4f-4981-8563-1683c4f34ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_validation(text_encoder, tokenizer, unet, vae, args, accelerator, weight_dtype, epoch):\n",
    "    \"\"\"\n",
    "    Runs validation and logs images to tensorboard and wandb.\n",
    "\n",
    "    Args:\n",
    "        text_encoder: The text encoder to use.\n",
    "        tokenizer: The tokenizer to use.\n",
    "        unet: The UNet model to use.\n",
    "        vae: The VAE model to use.\n",
    "        args: The arguments to use.\n",
    "        accelerator: The accelerator to use.\n",
    "        weight_dtype: The data type to use for the weights.\n",
    "        epoch: The current epoch.\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n",
    "        f\" {args.validation_prompt}.\"\n",
    "    )\n",
    "    # create pipeline (note: unet and vae are loaded again in float32)\n",
    "    pipeline = DiffusionPipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path,\n",
    "        text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "        tokenizer=tokenizer,\n",
    "        unet=accelerator.unwrap_model(unet),\n",
    "        vae=vae,\n",
    "        # revision=args.revision,\n",
    "        # torch_dtype=weight_dtype,\n",
    "        safety_checker=None,\n",
    "    )\n",
    "    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "    pipeline = pipeline.to(accelerator.device)\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    # run inference\n",
    "    generator = None if args.seed is None else torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "    images = []\n",
    "    for _ in range(args.num_validation_images):\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            image = pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]\n",
    "        images.append(image)\n",
    "        \n",
    "    for tracker in accelerator.trackers:\n",
    "        tracker.log(\n",
    "            {\n",
    "                \"validation\": [\n",
    "                    wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\") for i, image in enumerate(images)\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    del pipeline\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98435eaf-f82f-44f4-ad9d-fa4d956b89e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_function():\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision, \n",
    "        log_with='wandb'\n",
    "    )\n",
    "     # Make one log on every process with the configuration for debugging.\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state, main_process_only=False)\n",
    "    \n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    # Generate class images if prior preservation is enabled.\n",
    "    if args.with_prior_preservation:\n",
    "        class_images_dir = Path(args.class_data_dir) # get path of class images\n",
    "        if not class_images_dir.exists(): # create folder if not exists\n",
    "            class_images_dir.mkdir(parents=True)\n",
    "        cur_class_images = len(list(class_images_dir.iterdir())) # count how many images there are\n",
    "\n",
    "        if cur_class_images < args.num_class_images: # if there are less images than needed\n",
    "            # define dtype for prior generation\n",
    "            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\n",
    "            if args.prior_generation_precision == \"fp32\":\n",
    "                torch_dtype = torch.float32\n",
    "            elif args.prior_generation_precision == \"fp16\":\n",
    "                torch_dtype = torch.float16\n",
    "            elif args.prior_generation_precision == \"bf16\":\n",
    "                torch_dtype = torch.bfloat16\n",
    "            pipeline = DiffusionPipeline.from_pretrained(\n",
    "                args.pretrained_model_name_or_path, # from same pretrained model as main model\n",
    "                torch_dtype=torch_dtype,\n",
    "                safety_checker=None, # no safety checker\n",
    "            )\n",
    "            pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "            pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "            num_new_images = args.num_class_images - cur_class_images # number of images still needed\n",
    "            logger.info(f\"Number of class images to sample: {num_new_images}.\")\n",
    "\n",
    "            sample_dataset = PromptDataset(args.class_prompt, num_new_images) # define prompt dataset, returns prompt and index as item\n",
    "            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n",
    "\n",
    "            sample_dataloader = accelerator.prepare(sample_dataloader) # dataloader for accelerator\n",
    "            pipeline.to(accelerator.device) # send pipeline to accelerator device\n",
    "\n",
    "            for example in tqdm(sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process):\n",
    "                images = pipeline(example[\"prompt\"], num_inference_steps=25).images # generate image(s) from prompt\n",
    "\n",
    "                for i, image in enumerate(images): # for each image in the batch\n",
    "                    hash_image = hashlib.sha1(image.tobytes()).hexdigest() # create hash as unique identifier\n",
    "                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\n",
    "                    image.save(image_filename)\n",
    "\n",
    "            del pipeline # free memory when done\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    ##END OF CLASS PRIOR PRESERVATION\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.pretrained_model_name_or_path,\n",
    "        subfolder=\"tokenizer\",\n",
    "        # revision=args.revision,\n",
    "        use_fast=False\n",
    "    )\n",
    "    # import correct text encoder class: only ROBERTA or CLIP\n",
    "    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path)\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    text_encoder = text_encoder_cls.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"text_encoder\",\n",
    "        # revision=args.revision\n",
    "    )\n",
    "\n",
    "    #vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\n",
    "    vae=AutoencoderKL.from_pretrained(\n",
    "                    args.pretrained_model_name_or_path,\n",
    "                    subfolder=\"vae\",\n",
    "                    # revision=args.revision\n",
    "                )\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"unet\",\n",
    "        # revision=args.revision\n",
    "    )\n",
    "\n",
    "     # freeze models\n",
    "    vae.requires_grad_(False)\n",
    "    if not args.train_text_encoder: # freeze text encoder if not training it\n",
    "        text_encoder.requires_grad_(False)\n",
    "        \n",
    "    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
    "    # as these models are only used for inference, keeping weights in full precision is not required.\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "        \n",
    "\n",
    "     # Move vae and text_encoder to device and cast to weight_dtype\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    if not args.train_text_encoder:\n",
    "        text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    # optimize GPU memory usage and more\n",
    "    if args.enable_xformers_memory_efficient_attention:\n",
    "        if is_xformers_available():\n",
    "            import xformers\n",
    "\n",
    "            xformers_version = version.parse(xformers.__version__)\n",
    "            if xformers_version == version.parse(\"0.0.16\"):\n",
    "                logger.warn(\n",
    "                    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n",
    "                )\n",
    "            unet.enable_xformers_memory_efficient_attention()\n",
    "            vae.enable_xformers_memory_efficient_attention()\n",
    "        else:\n",
    "            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n",
    "    \n",
    "    if args.gradient_checkpointing:\n",
    "        unet.enable_gradient_checkpointing()\n",
    "        if args.train_text_encoder:\n",
    "            text_encoder.gradient_checkpointing_enable()\n",
    "            \n",
    "    # # Check that all trainable models are in full precision\n",
    "    # low_precision_error_string = (\n",
    "    #     \"Please make sure to always have all model weights in full float32 precision when starting training - even if\"\n",
    "    #     \" doing mixed precision training. copy of the weights should still be float32.\"\n",
    "    # )\n",
    "    # if accelerator.unwrap_model(unet).dtype != torch.float32:\n",
    "    #     raise ValueError(\n",
    "    #         f\"Unet loaded as datatype {accelerator.unwrap_model(unet).dtype}. {low_precision_error_string}\"\n",
    "    #     )\n",
    "    # if args.train_text_encoder and accelerator.unwrap_model(text_encoder).dtype != torch.float32:\n",
    "    #     raise ValueError(\n",
    "    #         f\"Text encoder loaded as datatype {accelerator.unwrap_model(text_encoder).dtype}.\"\n",
    "    #         f\" {low_precision_error_string}\"\n",
    "    #     )\n",
    "        \n",
    "    \n",
    "    # Enable TF32 for faster training on Ampere GPUs,\n",
    "    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "    if args.allow_tf32:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
    "    if args.use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n",
    "            )\n",
    "\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    params_to_optimize = ( # unet and text encoder or une only\n",
    "        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\n",
    "    )\n",
    "    optimizer = optimizer_class(\n",
    "        params_to_optimize,\n",
    "        lr=args.learning_rate,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "    )\n",
    "    \n",
    "    train_dataset = DreamBoothDataset(\n",
    "        train, \n",
    "        tokenizer,\n",
    "        class_data_root=args.class_data_dir if args.with_prior_preservation else None, # prior images\n",
    "        class_prompt=args.class_prompt, # class (prior) promt\n",
    "        class_num=args.num_class_images,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\n",
    "        num_workers = 8\n",
    "    )\n",
    "    \n",
    "    # Scheduler and math around the number of training steps.\n",
    "    overrode_max_train_steps = False\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if args.max_train_steps is None: # if not maximum number of steps is given\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "        overrode_max_train_steps = True\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        args.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps, # if acc steps are present, we need more warm up\n",
    "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps, # if more warm up is given, we need more training steps too.\n",
    "        num_cycles=args.lr_num_cycles,\n",
    "        power=args.lr_power,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    if args.train_text_encoder:\n",
    "        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n",
    "        )\n",
    "    else:\n",
    "        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "            unet, optimizer, train_dataloader, lr_scheduler\n",
    "        )\n",
    "        \n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / args.gradient_accumulation_steps\n",
    "    )\n",
    "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "    \n",
    "    # We need to initialize the trackers we use, and also store our configuration.\n",
    "    # The trackers initializes automatically on the main process.\n",
    "    if accelerator.is_main_process:\n",
    "        run = \"deambooth_mammo_emory_1\"\n",
    "        accelerator.init_trackers(run, config=vars(args)) # add args to wandb\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = (\n",
    "        args.train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * args.gradient_accumulation_steps\n",
    "    )\n",
    "    \n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    first_epoch = 0\n",
    "    \n",
    "    # Only show the progress bar once on each machine\n",
    "    progress_bar = tqdm(\n",
    "        range(args.max_train_steps), disable=not accelerator.is_local_main_process\n",
    "    )\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        unet.train()\n",
    "        if args.train_text_encoder:\n",
    "            text_encoder.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn(latents.shape).to(latents.device)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (bsz,),\n",
    "                    device=latents.device,\n",
    "                ).long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # Get the text embedding for conditioning\n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "                 # Predict the noise residual\n",
    "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "                # Get the target for loss depending on the prediction type\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "                    \n",
    "                if args.with_prior_preservation:\n",
    "                    # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\n",
    "                    try:\n",
    "                        model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "                        target, target_prior = torch.chunk(target, 2, dim=0)\n",
    "\n",
    "                        # Compute instance loss\n",
    "                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "                        # Compute prior loss\n",
    "                        prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
    "\n",
    "                        # Add the prior loss to the instance loss.\n",
    "                        loss = loss + args.prior_loss_weight * prior_loss\n",
    "                    except Exception as e:\n",
    "                        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "                else:\n",
    "                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    params_to_clip = (\n",
    "                        itertools.chain(unet.parameters(), text_encoder.parameters())\n",
    "                        if args.train_text_encoder\n",
    "                        else unet.parameters()\n",
    "                    )\n",
    "                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                \n",
    "                if accelerator.is_main_process:\n",
    "                    if args.validation_prompt is not None and global_step % args.validation_steps == 0: # log images to check progress\n",
    "                        log_validation(text_encoder, tokenizer, unet, vae, args, accelerator, weight_dtype, epoch)\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    # Create the pipeline using the trained modules and save it\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Loading pipeline and saving to {args.output_dir}...\")\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            args.pretrained_model_name_or_path,\n",
    "            unet=accelerator.unwrap_model(unet),\n",
    "            text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "            vae=AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\"),\n",
    "            safety_checker=None,\n",
    "        )\n",
    "        pipeline.save_pretrained(args.output_dir)\n",
    "        accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9305f7-9166-45e7-807c-7c0657a09e56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "num_of_gpus = 1  # CHANGE THIS TO MATCH THE NUMBER OF GPUS YOU HAVE\n",
    "notebook_launcher(\n",
    "    training_function, num_processes=num_of_gpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d308ea-b579-4e2c-8fd1-c7e78c2131b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe = StableDiffusionPipeline.from_pretrained(\n",
    "#     args.output_dir,\n",
    "#     torch_dtype=torch.float16,\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(args.output_dir, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "generator = torch.Generator(device='cuda')\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf416c-643d-4893-82ba-d5c13ac0f3f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_row(imgs, figsize=(10, 10), title=\"\", **kwargs):\n",
    "    \"\"\"Show generated images in a row\n",
    "\n",
    "    Args:\n",
    "        imgs (list): images list (PILs or numpy arrays)\n",
    "        figsize (tuple, optional): figure size. Defaults to (10, 10).\n",
    "        title (str, optional): title. Defaults to \"\".\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, len(imgs), figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "    for i, img in enumerate(imgs):\n",
    "        axs[i].imshow(img, **kwargs)\n",
    "        axs[i].axis(\"off\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356fa551-c3bd-47b1-b7ba-83eeb906906e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = random.sample(range(test.shape[0]), 2)\n",
    "prompt = test['prompt'].values[idx]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf681136-7eb9-4f47-8366-221bf5d0a444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pick a funny prompt here and it will be used as the widget's default \n",
    "# when we push to the Hub in the next section\n",
    "\n",
    "prompts = list(test['prompt'].values[idx])\n",
    "image_path = test['image_paths'].values[idx][0]\n",
    "\n",
    "# Tune the guidance to control how closely the generations follow the prompt\n",
    "# Values between 7-11 usually work best\n",
    "guidance_scale = [6, 7, 8, 9, 10, 11]\n",
    "\n",
    "for i in guidance_scale:    \n",
    "    for prompt in prompts:\n",
    "        # Seed\n",
    "        for seed in [1338]:\n",
    "            generator.manual_seed(seed)\n",
    "            negative_prompt = \"\"\n",
    "            num_samples = 2\n",
    "            num_inference_steps = 30\n",
    "            size = 512\n",
    "            with torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "                image = pipe(\n",
    "                    prompt=prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    num_images_per_prompt=num_samples,\n",
    "                    num_inference_steps=num_inference_steps,\n",
    "                    guidance_scale=i,\n",
    "                    height=size,\n",
    "                    width=size,\n",
    "                    generator=generator,\n",
    "                ).images\n",
    "            # images in list\n",
    "            imgs = [np.asarray(img) for img in image]\n",
    "\n",
    "            show_row(imgs, title=f'Guidance: {i}, prompt: {prompt}', figsize=(20,5), cmap=\"gray\") if len(imgs) > 1 else plt.imshow(imgs[0], cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa2974-57ce-40b6-8319-a6f33bbc4f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f86a9-e79f-4fa0-87c4-38d153dc9a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(mam)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
